{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "### 1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}  # maps word to integer representing it\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "  global word_encoding\n",
    "\n",
    "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
    "  bag = {}  # stores all of the encodings and their frequency\n",
    "\n",
    "  for word in words:\n",
    "    if word in vocab:\n",
    "      encoding = vocab[word]  # get encoding from vocab\n",
    "    else:\n",
    "      vocab[word] = word_encoding\n",
    "      encoding = word_encoding\n",
    "      word_encoding += 1\n",
    "    \n",
    "    if encoding in bag:\n",
    "      bag[encoding] += 1\n",
    "    else:\n",
    "      bag[encoding] = 1\n",
    "  \n",
    "  return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words loses the order in which words appear. When this happens, 2 sentences (as below) that have opposite meanings are taken to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
      "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_bag = bag_of_words(positive_review)\n",
    "neg_bag = bag_of_words(negative_review)\n",
    "\n",
    "print(\"Positive:\", pos_bag)\n",
    "print(\"Negative:\", neg_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM\n",
    "Sentiment analysis of moview reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset\n",
    "We'll use the IMDB movie review dataset from keras - containing 25,000 reviews where each is already preprocessed and has a label as either 'positive' or 'negative'. <br>\n",
    "Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 88584 # no. of unique words\n",
    "MAXLEN = 250 # length of longest review (no. of words)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)\n",
    "\n",
    "# Let's look at 1 review\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing\n",
    "We have to make all our reviews be of equal lengths  before we pass them into neural networks.<br>\n",
    "So we follow the procedure below:\n",
    "- if review > 250 words, trim off extra words\n",
    "- if review < 250 words, pad remaining spaces with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32), # 32 stands for vector dimensions for each word (but this can be changed)\n",
    "    tf.keras.layers.LSTM(32), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    # 'sigmoid' cuz result has to be between 0 & 1\n",
    "    # >0.5 is +ve\n",
    "    # <0.5 is -ve\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 94s 140ms/step - loss: 0.4175 - acc: 0.8108 - val_loss: 0.3082 - val_acc: 0.8776\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 80s 128ms/step - loss: 0.2409 - acc: 0.9085 - val_loss: 0.2866 - val_acc: 0.8788\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 84s 135ms/step - loss: 0.1840 - acc: 0.9323 - val_loss: 0.2704 - val_acc: 0.8954\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 91s 146ms/step - loss: 0.1499 - acc: 0.9471 - val_loss: 0.3530 - val_acc: 0.8852\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.1290 - acc: 0.9537 - val_loss: 0.3544 - val_acc: 0.8878\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 84s 134ms/step - loss: 0.1153 - acc: 0.9593 - val_loss: 0.3073 - val_acc: 0.8902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.0985 - acc: 0.9665 - val_loss: 0.3332 - val_acc: 0.8788\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 79s 127ms/step - loss: 0.0885 - acc: 0.9710 - val_loss: 0.3878 - val_acc: 0.8868\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 77s 123ms/step - loss: 0.0778 - acc: 0.9743 - val_loss: 0.4288 - val_acc: 0.8822\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 79s 127ms/step - loss: 0.0720 - acc: 0.9779 - val_loss: 0.3548 - val_acc: 0.8848\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "history = model.fit(\n",
    "    train_data, train_labels, epochs=10, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 44s 55ms/step - loss: 0.4179 - acc: 0.8602\n",
      "[0.4178619980812073, 0.8602399826049805]\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"4a) LSTM.h5\")\n",
    "new_model = tf.keras.models.load_model('4a) LSTM.h5')\n",
    "\n",
    "results = new_model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lookup table - word-to-int mapping\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    # tokens are the individual words themselves\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    # create a for loop: if word is in mapping/vocabulary, replace with integer that represents it, otherwise put 0 (ie. we don't know what that word means)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "# in case we have any movie reviews in code format\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "def decode_ints(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "    return text[:-1] # return everything except the last space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n",
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "#text = input(\"Please enter a movie review (string):\")\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)\n",
    "print(decode_ints(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a large bunch of zeroes, they might just be padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91993374]\n",
      "[0.3243059]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250))\n",
    "    pred[0] = encoded_text\n",
    "    result = new_model.predict(pred)\n",
    "    print(result[0])\n",
    "\n",
    "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great!\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched.\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.292518]\n"
     ]
    }
   ],
   "source": [
    "predict(\"and bad bad bad bad bad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RNN Play Generator\n",
    "Predict the next character in a play's script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset\n",
    "To train our neural network, we'll use an extract from a Shakespeare play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load your own data instead, you can run the code below. <br>\n",
    "It allows you t upload a text file from a dialog box. <br>\n",
    "<b>But this code only works on Google Colab!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and decode file\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# Length of text = no. of characters in it\n",
    "print(f\"Length of text: {len(text)}\\n\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters in text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# Create a mapping from unqiue characters to indices\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "# Reverse mapping\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# convert our string into an array of indices\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)\n",
    "\n",
    "# Let's look at how our text is encoded\n",
    "print(f\"Text: {text[:13]}\")\n",
    "print(f\"Encoded: {text_to_int(text[:13])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "# While we're at it, here's a function that converts numeric values to text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Training Examples\n",
    "Now we'll splitour text data from above to manu shorter sequences that we can pass to the model as traning examples. <br>\n",
    "\n",
    "Sample I/O:<br>\n",
    "```input: 'Hell' | output: 'ello'```\n",
    "\n",
    "We first create a stream of chars from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 # no. of chars per sequence\n",
    "examples_per_epoch = len(text)//(seq_length)\n",
    "\n",
    "# Create traininhg examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the stream of chars into batches of 101 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split these sequences of 101 chars into input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # 'hell'\n",
    "    target_text = chunk[1:] # 'ello'\n",
    "    return input_text, target_text\n",
    "\n",
    "# we use map to apply the above function to every entry\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for x,y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 64 training examples\n",
    "VOCAB_SIZE = len(vocab) # vocab = no. of unique chars in each example\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # None = we don't know how many sequences are there\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        # last layer produces a probability distribution of all characters, showing likelihood of each appering next\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a loss function\n",
    "But before that, let's look at a sample input & output from our untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    # ask our model for a prediction on 1st batch/training example\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    # where vocab_size of 65 is for probability distribution of all 65 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-5.5902796e-03  4.8362846e-03  3.5653710e-03 ... -1.0484520e-03\n",
      "    7.2944192e-03  4.3258257e-03]\n",
      "  [-1.7047012e-03 -2.9690573e-03 -1.0410429e-03 ...  2.1018782e-03\n",
      "    3.8197571e-03  1.0282792e-02]\n",
      "  [-6.3605262e-03  3.1918108e-03  2.0015314e-03 ...  4.5323637e-03\n",
      "   -7.0677249e-04  3.3347616e-03]\n",
      "  ...\n",
      "  [ 6.6365935e-03  4.9180533e-03 -8.3896853e-03 ... -4.1031064e-03\n",
      "   -1.0784393e-02  5.8393274e-03]\n",
      "  [-3.0406867e-04  8.7803323e-03 -4.0227887e-03 ...  2.0392612e-04\n",
      "   -1.2119182e-02 -1.8691644e-05]\n",
      "  [ 8.4787328e-03  5.3616515e-03 -7.3989010e-03 ...  4.3986426e-03\n",
      "   -1.3393762e-02 -3.0823543e-03]]\n",
      "\n",
      " [[-2.2007397e-03 -5.5384487e-03 -2.5872269e-03 ... -1.3170720e-03\n",
      "    2.8870418e-03 -4.7442247e-03]\n",
      "  [ 2.7678423e-03 -6.0960004e-04 -5.7020513e-03 ...  1.1706905e-04\n",
      "    3.7765643e-04 -3.8884350e-03]\n",
      "  [ 7.0683425e-03  2.2395994e-03 -8.9842724e-03 ...  7.4156706e-04\n",
      "   -1.5109024e-03 -2.2601876e-03]\n",
      "  ...\n",
      "  [-1.8869747e-03  5.4034386e-03 -1.1454528e-03 ...  8.6066565e-03\n",
      "   -1.1898888e-02 -9.7427145e-03]\n",
      "  [ 6.6318400e-03  1.0833641e-03 -5.0337138e-03 ...  9.2959339e-03\n",
      "   -1.2461088e-02 -1.0229450e-02]\n",
      "  [-3.9416458e-03 -3.9359345e-03 -3.6052186e-03 ...  6.9998149e-03\n",
      "   -8.9585101e-03 -8.7117357e-03]]\n",
      "\n",
      " [[ 2.5654475e-03  1.8739854e-03  6.2429169e-03 ...  5.0175260e-03\n",
      "   -7.4051828e-03  2.6924922e-03]\n",
      "  [ 4.6021785e-03  5.0355224e-03  1.0210993e-03 ... -2.4565835e-03\n",
      "   -6.8098265e-03 -2.7776412e-03]\n",
      "  [ 4.1364487e-03  3.5582355e-03  1.8979334e-03 ...  1.6516722e-03\n",
      "    7.3903857e-04 -2.2683898e-04]\n",
      "  ...\n",
      "  [ 5.6840829e-03 -4.1210577e-03 -4.2654094e-03 ...  1.8320940e-03\n",
      "   -3.1120027e-03 -1.4023336e-03]\n",
      "  [ 4.0724725e-03 -1.8574474e-03 -4.0015941e-03 ... -7.3989932e-03\n",
      "   -9.6132839e-03  6.1710491e-03]\n",
      "  [-8.7519133e-05 -6.3436734e-03 -5.0377250e-03 ... -6.7186682e-03\n",
      "   -5.0104223e-03  9.1042102e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.9071903e-04  1.8472419e-04  2.1069632e-03 ...  3.6613939e-03\n",
      "    5.3604553e-03  2.2958077e-03]\n",
      "  [-8.3492808e-03 -5.2424101e-03  1.0931119e-04 ...  2.8688535e-03\n",
      "    4.6131620e-03  5.5277988e-04]\n",
      "  [-4.7564059e-03 -1.2962313e-03 -4.6409895e-03 ... -2.9329795e-03\n",
      "    6.3815960e-03  2.0849114e-04]\n",
      "  ...\n",
      "  [ 9.3271742e-03 -1.4281659e-03 -9.2810951e-03 ...  3.6454252e-03\n",
      "   -8.0687003e-03  9.4251265e-04]\n",
      "  [ 6.2952582e-03 -7.6555065e-04 -5.7722270e-03 ...  6.9253347e-03\n",
      "   -1.7319414e-03  2.2858526e-03]\n",
      "  [ 1.2429951e-02 -2.8767514e-03 -9.7779427e-03 ...  8.9267874e-03\n",
      "   -5.4969396e-03 -1.0986757e-03]]\n",
      "\n",
      " [[ 8.2693640e-03 -1.9793184e-03 -4.1095661e-03 ...  3.5066842e-03\n",
      "   -3.5840292e-03 -2.2806670e-03]\n",
      "  [ 9.7725319e-04  3.5531949e-03  1.2542896e-03 ...  5.7592085e-03\n",
      "   -5.7999422e-03 -4.9045482e-03]\n",
      "  [-2.4628658e-03  3.8373184e-03  1.2016497e-03 ...  6.2238565e-03\n",
      "   -6.5699145e-03  6.0263462e-04]\n",
      "  ...\n",
      "  [ 7.4546458e-03 -6.9798212e-03 -9.6518882e-03 ... -2.2175983e-03\n",
      "   -3.7061349e-03  2.9350678e-04]\n",
      "  [-3.2022535e-03 -9.9496916e-03 -6.5657897e-03 ... -1.2892117e-03\n",
      "   -2.0917794e-03  1.8927199e-04]\n",
      "  [-4.2061661e-03 -1.2765228e-02 -6.8411785e-03 ... -2.6455321e-03\n",
      "    2.8970488e-03 -4.2932150e-03]]\n",
      "\n",
      " [[-2.3052860e-03 -2.6056976e-03  2.2780085e-03 ... -4.6353843e-03\n",
      "    5.9182821e-03  1.0446643e-03]\n",
      "  [ 3.1067436e-03  1.7622118e-03 -3.5766221e-03 ... -3.6261966e-03\n",
      "    1.6314634e-03  1.2628975e-03]\n",
      "  [ 1.2907841e-03 -4.9252077e-03 -2.3556254e-03 ... -6.1795777e-03\n",
      "   -1.1816970e-03 -1.6915472e-04]\n",
      "  ...\n",
      "  [-8.4924502e-03  6.6208071e-04 -1.4676894e-03 ... -2.1768599e-03\n",
      "    1.9380165e-02 -4.3275012e-03]\n",
      "  [ 1.3683089e-03 -2.7246936e-04 -5.2342047e-03 ...  1.5707780e-03\n",
      "    1.1921855e-02 -5.4963408e-03]\n",
      "  [-3.4430739e-03  2.3261420e-03 -3.7273234e-03 ...  2.8087455e-03\n",
      "    7.6866788e-03  4.8884086e-04]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediction is an array of 64 arrays, each array represents 1 batch/training example\n",
    "print(len(example_batch_predictions)) # batch_size\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-5.5902796e-03  4.8362846e-03  3.5653710e-03 ... -1.0484520e-03\n",
      "   7.2944192e-03  4.3258257e-03]\n",
      " [-1.7047012e-03 -2.9690573e-03 -1.0410429e-03 ...  2.1018782e-03\n",
      "   3.8197571e-03  1.0282792e-02]\n",
      " [-6.3605262e-03  3.1918108e-03  2.0015314e-03 ...  4.5323637e-03\n",
      "  -7.0677249e-04  3.3347616e-03]\n",
      " ...\n",
      " [ 6.6365935e-03  4.9180533e-03 -8.3896853e-03 ... -4.1031064e-03\n",
      "  -1.0784393e-02  5.8393274e-03]\n",
      " [-3.0406867e-04  8.7803323e-03 -4.0227887e-03 ...  2.0392612e-04\n",
      "  -1.2119182e-02 -1.8691644e-05]\n",
      " [ 8.4787328e-03  5.3616515e-03 -7.3989010e-03 ...  4.3986426e-03\n",
      "  -1.3393762e-02 -3.0823543e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's examine 1 prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred)) # sequence_length\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-5.59027959e-03  4.83628456e-03  3.56537104e-03  2.09279777e-03\n",
      "  4.54792171e-05 -1.45385892e-03  1.65328197e-03  1.27973763e-04\n",
      "  3.75822652e-03 -1.03796087e-02 -5.54984296e-03 -4.39971173e-03\n",
      "  2.83340749e-04 -2.92270281e-03  1.36586127e-03 -1.53386383e-03\n",
      " -2.19126651e-03  3.11579416e-03  4.60878538e-04  3.44331958e-03\n",
      " -2.28811894e-03 -3.36171919e-03  2.25809286e-04 -3.94706498e-04\n",
      " -1.41388667e-03  1.87411194e-03 -7.91624538e-04  3.65795754e-03\n",
      " -1.78508949e-03  2.33548600e-03 -2.04534410e-03  1.88651215e-03\n",
      " -7.26399058e-03 -3.48568219e-03  2.99124047e-04  2.46205530e-03\n",
      "  1.62493961e-03 -1.12140086e-04  7.09834509e-03 -6.70532137e-03\n",
      " -4.16385708e-03  6.42121397e-03 -1.67269888e-03  4.28609410e-03\n",
      " -5.69081749e-04 -2.36260891e-03  7.34823663e-03  3.16024642e-04\n",
      "  3.52315721e-03  1.83605298e-03 -1.16388861e-03 -3.16537032e-03\n",
      " -2.14437139e-03 -7.45764410e-04 -4.25127381e-03 -4.38400637e-03\n",
      " -2.76780291e-03 -2.71927100e-03  4.16678726e-04  4.30657621e-03\n",
      "  5.93875814e-03  3.33228381e-05 -1.04845199e-03  7.29441922e-03\n",
      "  4.32582572e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediction at 1st time step\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred)) # vocab_size\n",
    "print(time_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LcW3KMf&3-fxdFGObwLcqoI;Yy-l.WWJiVvy;cJuuoEwBMFmhEdrOx;NaYGp$yMTF:R?:;mETuG'R&3ILT.,'FOn.fzNWnLnMDwe\n"
     ]
    }
   ],
   "source": [
    "# To get predicted character, we sample the output distribution (pick a char based on probability)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# Reshape array & convery all ints to numbers to see actual chars\n",
    "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "print(predicted_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model is untrained, what you see above is stream of predicted chars based on random weights & biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create checkpoints\n",
    "Setup and configure model to save checkpoint as it trains. This allows loading model from a checkpoint & continue training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where checkpoints will be saved\n",
    "checkpoint_dir = './4b) training_checkpoints'\n",
    "# Name of checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model\n",
    "We'll rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one peice of text to the model and have it make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model finishes training, we can find the latest checkpoint that stores the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    # We reset model's state so that it forgets any previous inputs\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloo have my mighty.\n",
      "\n",
      "GLOUCESTER:\n",
      "Alas, thou discover you my lightning: let him\n",
      "gances, his nature, cry 'Congation.\n",
      "\n",
      "Mester:\n",
      "The prince till my bestrew'd in the Back\n",
      "Ceter to the Tower. Sweet like an honour\n",
      "Myself than other world,\n",
      "Comes to her never heard the gods; since what we are,\n",
      "To court her with his sorrow.\n",
      "\n",
      "First Senator:\n",
      "Say they will stand do't in Angelo.\n",
      "\n",
      "JULIET:\n",
      "Cartoll anon-merce, you have seem\n",
      "consorted stoke: pish use and weeping men,\n",
      "And for the Benevil to the Duke of York\n",
      "Is strake us to our honour in Rome,\n",
      "Then before weeping seized to the great father of spect.\n",
      "Gorrewlore.\n",
      "\n",
      "Third Musician:\n",
      "Why, thou art a brating auterors, lawful\n",
      "To be bestock fair and perish!\n",
      "Shamely thou wait, or breathe my heavens!\n",
      "Say\n",
      "you know'st thou went? thee at the meray that?\n",
      "Ibf why, so do you nea\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
